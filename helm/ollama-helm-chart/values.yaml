# Ollama configuration for local LLM support
ollama:
  # Ollama service configuration
  service:
    type: ClusterIP
    port: 11434
  # Environment variables for performance optimization
  extraEnv:
    - name: OLLAMA_KEEP_ALIVE
      value: "-1"  # Keep models in memory indefinitely
    - name: OLLAMA_NUM_PARALLEL
      value: "4"   # Allow parallel requests
    - name: OLLAMA_MAX_LOADED_MODELS
      value: "3"   # Keep up to 3 models in memory
  # Resource limits for Ollama (optimized for performance)
  resources:
    limits:
      memory: "6Gi"  # Increased for keeping models in memory
      cpu: "4"
    requests:
      memory: "3Gi"  # Increased baseline
      cpu: "2"       # Higher CPU request for better scheduling
  # Persistent Volume for model storage
  persistentVolume:
    enabled: true
    size: 15Gi     # Increased for multiple models
    storageClass: ""  # Use default storage class
  # Models to pull and preload at startup
  models:
    pull:
      - llama3.2:3b
      - smollm2:1.7b
      - smollm2:360m
    run:
      - llama3.2:3b   # Preload primary model into memory
      - smollm2:1.7b  # Preload backup models
  # GPU support (set to true if you have GPU nodes)
  gpu:
    enabled: false
    type: 'nvidia'
    number: 1
  # Liveness and readiness probes
  livenessProbe:
    enabled: true
    initialDelaySeconds: 60   # Allow time for model loading
    periodSeconds: 30
    timeoutSeconds: 10
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
  # Node selector for GPU nodes (if using GPU)
  nodeSelector: {}
  # Tolerations for GPU nodes
  tolerations: []
  # Namespace
  namespace: bnfr